````markdown
# ğŸ“¹ Surveillance VidÃ©o AvancÃ©e avec YOLO  
*DÃ©tection dâ€™objets en temps rÃ©el pour une sÃ©curitÃ© proactive*

Ce projet implÃ©mente un systÃ¨me de surveillance vidÃ©o en temps rÃ©el basÃ© sur le modÃ¨le **YOLO (You Only Look Once)**.  
Il dÃ©tecte et suit des objets â€” principalement des personnes â€” dans une zone dâ€™intÃ©rÃªt prÃ©dÃ©finie, offrant une solution rapide, prÃ©cise et rÃ©active, idÃ©ale pour les environnements exigeant une vigilance accrue.

---

## ğŸ“œ Description & UtilitÃ©  

Ce systÃ¨me analyse un flux vidÃ©o via des techniques avancÃ©es de vision par ordinateur et apprentissage automatique. Il dÃ©tecte les objets et signale leur prÃ©sence dans une zone configurÃ©e par un polygone.  
Applications principales :  
- ğŸ¢ Surveillance de parkings, accÃ¨s et zones sensibles  
- ğŸš¨ SÃ©curitÃ© publique proactive et prÃ©vention dâ€™incidents  
- â±ï¸ Analyse vidÃ©o en temps rÃ©el pour des interventions rapides  

---

## âœ¨ FonctionnalitÃ©s clÃ©s  

- ğŸ•µï¸â€â™‚ï¸ **DÃ©tection dâ€™objets** (personnes, vÃ©hicules, etc.) en temps rÃ©el avec YOLOv8  
- ğŸ“ **Zone dâ€™intÃ©rÃªt paramÃ©trable** via polygone affichÃ© en magenta  
- ğŸ¨ **Visualisation intuitive** avec cadres rouges autour des objets dans la zone  
- ğŸ–±ï¸ **Interaction souris** : affichage en console des coordonnÃ©es et valeurs BGR des pixels survolÃ©s  
- ğŸ“¡ **Traitement frame par frame** depuis fichier vidÃ©o ou flux en direct  

---

## ğŸ› ï¸ PrÃ©requis  

- OS : Windows, macOS ou Linux  
- Python 3.8+  
- BibliothÃ¨ques Python :  
  - `opencv-python`  
  - `ultralytics`  
  - `pandas`  
  - `numpy`  
- ModÃ¨le YOLO (`yolov8s.pt`) disponible localement ou tÃ©lÃ©chargÃ© automatiquement  
- Fichier classes COCO (`coco.txt`) dans le dossier projet  
- VidÃ©o `.mp4` (ex : `VIDEO.mp4`)  

---

## ğŸš€ Installation & dÃ©marrage rapide  

1. Cloner le dÃ©pÃ´t :  
   ```bash
   git clone https://github.com/HaMzaCheh/Advanced-Video-Surveillance-YOLO.git
   cd Advanced-Video-Surveillance-YOLO
````

2. CrÃ©er et activer un environnement virtuel (fortement recommandÃ©) :

   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows : venv\Scripts\activate
   ```

3. Installer les dÃ©pendances :

   ```bash
   pip install opencv-python ultralytics pandas numpy
   ```

4. PrÃ©parer les fichiers :

   * Placer `coco.txt` dans le dossier du projet
   * Mettre la vidÃ©o `.mp4` (ex : `VIDEO.mp4`) dans le dossier ou ajuster son chemin dans `main.py`

5. Lancer lâ€™application :

   ```bash
   python main.py
   ```

---

## ğŸ–¥ï¸ Utilisation

* La fenÃªtre `Ecran_detection` sâ€™ouvre et affiche la vidÃ©o avec :

  * Cadres rouges autour des objets dÃ©tectÃ©s **dans la zone dâ€™intÃ©rÃªt**
  * Zone dâ€™intÃ©rÃªt tracÃ©e en magenta
* Survolez la fenÃªtre avec la souris pour voir dans la console coordonnÃ©es et valeurs BGR des pixels
* Appuyez sur `ESC` pour quitter
* Modifiez la variable `area` dans `main.py` pour changer la zone surveillÃ©e

---

## ğŸ§  Architecture & fonctionnement

* Chargement du modÃ¨le YOLOv8 via `ultralytics`
* Lecture vidÃ©o frame par frame avec OpenCV
* DÃ©tection des objets dans chaque frame
* VÃ©rification de la prÃ©sence dans la zone via `cv2.pointPolygonTest`
* Affichage des cadres et de la zone dâ€™intÃ©rÃªt
* Gestion des Ã©vÃ©nements souris

Le code est modulaire et adaptable Ã  diffÃ©rents flux ou zones.

---

## ğŸ¥ DÃ©monstration / Simulation

* Lancez `python main.py` pour visualiser la dÃ©tection en temps rÃ©el
* Testez diffÃ©rentes vidÃ©os ou modifiez la zone dâ€™intÃ©rÃªt
* RecommandÃ© : vidÃ©o avec personnes dans un environnement surveillÃ© (parking, entrÃ©e, etc.)

---

## ğŸ¤ Contribution

Contributions bienvenues !

* Forkez le projet
* CrÃ©ez une branche :

  ```bash
  git checkout -b ma-nouvelle-fonctionnalite
  ```
* Modifiez et testez
* Soumettez une pull request claire

Merci de respecter PEP 8 et dâ€™inclure des tests si possible.

---

ğŸ’¬ **Merci pour votre intÃ©rÃªt ! Vos retours sont essentiels pour amÃ©liorer ce projet ğŸš€**
Nâ€™hÃ©sitez pas Ã  ouvrir une issue ou me contacter via GitHub.
